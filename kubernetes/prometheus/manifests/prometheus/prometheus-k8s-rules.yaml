apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: prometheus-k8s-rules-system
  labels:
    role: prometheus-rulefiles
    prometheus: k8s
    alertmanager-rules-group: common
spec:
  groups:
  - name: kubernetes-absent
    rules:
    - alert: AlertmanagerDown
      annotations:
        description: Alertmanager has disappeared from Prometheus target discovery.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-alertmanagerdown
      expr: |
        absent(up{job="alertmanager-main"} == 1)
      for: 15m
      labels:
        severity: '3'
    - alert: KubeAPIDown
      annotations:
        description: KubeAPI has disappeared from Prometheus target discovery.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapidown
      expr: |
        absent(up{job="apiserver"} == 1)
      for: 15m
      labels:
        severity: '3'
    - alert: KubeStateMetricsDown
      annotations:
        description: KubeStateMetrics has disappeared from Prometheus target discovery.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatemetricsdown
      expr: |
        absent(up{job="kube-state-metrics"} == 1)
      for: 15m
      labels:
        severity: '3'
    - alert: KubeletDown
      annotations:
        description: Kubelet has disappeared from Prometheus target discovery.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletdown
      expr: |
        absent(up{job="kubelet"} == 1)
      for: 15m
      labels:
        severity: '3'
    - alert: NodeExporterDown
      annotations:
        description: NodeExporter has disappeared from Prometheus target discovery.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodeexporterdown
      expr: |
        absent(up{job="node-exporter"} == 1)
      for: 15m
      labels:
        severity: '3'
    - alert: PrometheusDown
      annotations:
        description: Prometheus has disappeared from Prometheus target discovery.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-prometheusdown
      expr: |
        absent(up{job="prometheus-k8s"} == 1)
      for: 15m
      labels:
        severity: '3'
    - alert: PrometheusOperatorDown
      annotations:
        description: PrometheusOperator has disappeared from Prometheus target discovery.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-prometheusoperatordown
      expr: |
        absent(up{job="prometheus-operator"} == 1)
      for: 15m
      labels:
        severity: '3'

  - name: kubernetes-apps
    rules:
    - alert: KubePodCrashLooping
      annotations:
        description: 'Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container
          }}) is restarting {{ printf "%.2f" $value }} times / 5 minutes.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodcrashlooping
      expr: |
        rate(kube_pod_container_status_restarts_total{job="kube-state-metrics"}[15m]) * on (namespace,pod) group_left(label_team) kube_pod_labels * 60 * 5 > 0
      for: 1h
      labels:
        severity: '3'
    - alert: KubePodNotReady
      annotations:
        description: 'Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready
          state for longer than an hour.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready
      expr: |
        sum by (namespace, pod) (kube_pod_status_phase{job="kube-state-metrics", phase=~"Pending|Unknown"}) * on (namespace,pod) group_left(label_team) kube_pod_labels > 0
      for: 1h
      labels:
        severity: '3'
    - alert: KubeDeploymentGenerationMismatch
      annotations:
        description: 'Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment
          }} does not match, this indicates that the Deployment has failed but has
          not been rolled back.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentgenerationmismatch
      expr: |
        (kube_deployment_status_observed_generation{job="kube-state-metrics"}
          !=
        kube_deployment_metadata_generation{job="kube-state-metrics"})
        * on(namespace,deployment) group_left(label_team) kube_deployment_labels
      for: 15m
      labels:
        severity: '3'
    - alert: KubeDeploymentReplicasMismatch
      annotations:
        description: 'Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not
          matched the expected number of replicas for longer than an hour.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentreplicasmismatch
      expr: |
        (kube_deployment_spec_replicas{job="kube-state-metrics"}
          !=
        kube_deployment_status_replicas_available{job="kube-state-metrics"})
        * on(namespace,deployment) group_left(label_team) kube_deployment_labels
      for: 1h
      labels:
        severity: '3'
    - alert: KubeStatefulSetReplicasMismatch
      annotations:
        description: 'StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has
          not matched the expected number of replicas for longer than 15 minutes.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetreplicasmismatch
      expr: |
        (kube_statefulset_status_replicas_ready{job="kube-state-metrics"}
          !=
        kube_statefulset_status_replicas{job="kube-state-metrics"})
        * on(namespace,statefulset) group_left(label_team) kube_statefulset_labels
      for: 15m
      labels:
        severity: '3'
    - alert: KubeStatefulSetGenerationMismatch
      annotations:
        description: 'StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset
          }} does not match, this indicates that the StatefulSet has failed but has
          not been rolled back.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetgenerationmismatch
      expr: |
        (kube_statefulset_status_observed_generation{job="kube-state-metrics"}
          !=
        kube_statefulset_metadata_generation{job="kube-state-metrics"})
        * on(namespace,statefulset) group_left(label_team) kube_statefulset_labels
      for: 15m
      labels:
        severity: '3'
    - alert: KubeStatefulSetUpdateNotRolledOut
      annotations:
        description: 'StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update
          has not been rolled out.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetupdatenotrolledout
      expr: |
        max without (revision) (
          kube_statefulset_status_current_revision{job="kube-state-metrics"}
            unless
          kube_statefulset_status_update_revision{job="kube-state-metrics"}
        )
          *
        (
          kube_statefulset_replicas{job="kube-state-metrics"}
            !=
          kube_statefulset_status_replicas_updated{job="kube-state-metrics"}
        ) * on(namespace,statefulset) group_left(label_team) kube_statefulset_labels
      for: 15m
      labels:
        severity: '3'
    - alert: KubeDaemonSetRolloutStuck
      annotations:
        description: 'Only {{ $value }}% of the desired Pods of DaemonSet {{ $labels.namespace
          }}/{{ $labels.daemonset }} are scheduled and ready.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetrolloutstuck
      expr: |
        (kube_daemonset_status_number_ready{job="kube-state-metrics"}
          /
        kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"})
         * on(namespace,daemonset) group_left(label_team) kube_daemonset_labels
         * 100 < 100
      for: 15m
      labels:
        severity: '3'
    - alert: KubeDaemonSetNotScheduled
      annotations:
        description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
          }} are not scheduled.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetnotscheduled
      expr: |
        (kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}
          -
        kube_daemonset_status_current_number_scheduled{job="kube-state-metrics"})
         * on(namespace,daemonset) group_left(label_team) kube_daemonset_labels > 0
      for: 10m
      labels:
        severity: '2'
    - alert: KubeDaemonSetMisScheduled
      annotations:
        description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
          }} are running where they are not supposed to run.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetmisscheduled
      expr: |
        kube_daemonset_status_number_misscheduled{job="kube-state-metrics"}
         * on(namespace,daemonset) group_left(label_team) kube_daemonset_labels > 0
      for: 10m
      labels:
        severity: '2'
    - alert: PodFrequentlyRestarting
      expr: |
        increase(kube_pod_container_status_restarts_total[1h])
         * on (namespace,pod) group_left(label_team) kube_pod_labels > 5
      for: 10m
      labels:
        severity: '2'
      annotations:
        description: 'Pod {{$labels.namespace}}/{{$labels.pod}} was restarted {{$value | printf "%.0f"}}
          times within the last hour'
        summary: Pod is restarting frequently
    - alert: PodFrequentlyRestarting
      expr: |
        increase(kube_pod_container_status_restarts_total[1h])
         * on (namespace,pod) group_left(label_team) kube_pod_labels > 10
      for: 10m
      labels:
        severity: '3'
      annotations:
        description: 'Pod {{$labels.namespace}}/{{$labels.pod}} was restarted {{$value | printf "%.0f"}}
          times within the last hour'
        summary: Pod is restarting frequently

  - name: kubernetes-resources
    rules:
    - alert: KubeCPUOvercommit
      annotations:
        description: Cluster has overcommitted CPU resource requests for Pods and cannot
          tolerate node failure.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit
      expr: |
        sum(namespace_name:kube_pod_container_resource_requests_cpu_cores:sum)
          /
        sum(node:node_num_cpu:sum)
          >
        (count(node:node_num_cpu:sum)-1) / count(node:node_num_cpu:sum)
      for: 5m
      labels:
        severity: '2'
    - alert: KubeMemOvercommit
      annotations:
        description: Cluster has overcommitted memory resource requests for Pods and cannot
          tolerate node failure.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit
      expr: |
        sum(namespace_name:kube_pod_container_resource_requests_memory_bytes:sum)
          /
        sum(node_memory_MemTotal)
          >
        (count(node:node_num_cpu:sum)-1)
          /
        count(node:node_num_cpu:sum)
      for: 5m
      labels:
        severity: '2'
    - alert: KubeCPUOvercommit
      annotations:
        description: Cluster has overcommitted CPU resource requests for Namespaces.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit
      expr: |
        sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="requests.cpu"})
          /
        sum(node:node_num_cpu:sum)
          > 1.5
      for: 5m
      labels:
        severity: '2'
    - alert: KubeMemOvercommit
      annotations:
        description: Cluster has overcommitted memory resource requests for Namespaces.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit
      expr: |
        sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="requests.memory"})
          /
        sum(node_memory_MemTotal{job="node-exporter"})
          > 1.5
      for: 5m
      labels:
        severity: '2'
    - alert: KubeQuotaExceeded
      annotations:
        description: 'Namespace {{ $labels.namespace }} is using {{ printf "%0.0f" $value
          }}% of its {{ $labels.resource }} quota.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubequotaexceeded
      expr: |
        100 * kube_resourcequota{job="kube-state-metrics", type="used"}
          / ignoring(instance, job, type)
        (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
          > 90
      for: 15m
      labels:
        severity: '2'
  - name: kubernetes-storage
    rules:
    - alert: KubePersistentVolumeUsageWarn10
      annotations:
        description: 'The PersistentVolume claimed by {{ $labels.persistentvolumeclaim
          }} in Namespace {{ $labels.namespace }} is only {{ printf "%0.0f" $value
          }}% free.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeusagecritical
      expr: |
        100 *
        (kubelet_volume_stats_available_bytes{job="kubelet"}
        /
        kubelet_volume_stats_capacity_bytes{job="kubelet"})
        * on(namespace,persistentvolumeclaim) group_left(label_team) kube_persistentvolumeclaim_labels
        < 10
      for: 1m
      labels:
        severity: '2'
    - alert: KubePersistentVolumeUsageCritical5
      annotations:
        description: 'The PersistentVolume claimed by {{ $labels.persistentvolumeclaim
          }} in Namespace {{ $labels.namespace }} is only {{ printf "%0.0f" $value
          }}% free.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeusagecritical
      expr: |
        100 *
        (kubelet_volume_stats_available_bytes{job="kubelet"}
        /
        kubelet_volume_stats_capacity_bytes{job="kubelet"})
        * on(namespace,persistentvolumeclaim) group_left(label_team) kube_persistentvolumeclaim_labels
        < 5
      for: 1m
      labels:
        severity: '3'
    - alert: KubePersistentVolumeUsageCritical3
      annotations:
        description: 'The PersistentVolume claimed by {{ $labels.persistentvolumeclaim
          }} in Namespace {{ $labels.namespace }} is only {{ printf "%0.0f" $value
          }}% free.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeusagecritical
      expr: |
        100 *
        (kubelet_volume_stats_available_bytes{job="kubelet"}
        /
        kubelet_volume_stats_capacity_bytes{job="kubelet"})
        * on(namespace,persistentvolumeclaim) group_left(label_team) kube_persistentvolumeclaim_labels
        < 3
      for: 1m
      labels:
        severity: '3'
    - alert: KubePersistentVolumeFullInFourDays
      annotations:
        description: 'Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim
          }} in Namespace {{ $labels.namespace }} is expected to fill up within four
          days. Currently {{ $value }} bytes are available.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumefullinfourdays
      expr: |
        ((
          kubelet_volume_stats_used_bytes{job="kubelet"}
            /
          kubelet_volume_stats_capacity_bytes{job="kubelet"}
        ) > 0.85
        and
        predict_linear(kubelet_volume_stats_available_bytes{job="kubelet"}[6h], 4 * 24 * 3600))
        * on(namespace,persistentvolumeclaim) group_left(label_team) kube_persistentvolumeclaim_labels
        < 0
      for: 5m
      labels:
        severity: '2'

  - name: kubernetes-system
    rules:
    - alert: KubeNodeNotReady
      annotations:
        description: '{{ $labels.node }} has been unready for more than an hour.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodenotready
      expr: |
        kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"} == 0
      for: 1h
      labels:
        severity: '2'
    - alert: KubeVersionMismatch
      annotations:
        description: 'There are {{ $value }} different versions of Kubernetes components
          running.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeversionmismatch
      expr: |
        count(count(kubernetes_build_info{job!="kube-dns", job!="apiserver"}) by (gitVersion)) > 1
      for: 1h
      labels:
        severity: '2'
    - alert: KubeClientErrors
      annotations:
        description: 'Kubernetes API server client "{{ $labels.job }}/{{ $labels.instance
          }}" is experiencing {{ printf "%0.0f" $value }}% errors.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclienterrors
      expr: |
        (sum(rate(rest_client_requests_total{code!~"2..|404"}[5m])) by (instance, job)
          /
        sum(rate(rest_client_requests_total[5m])) by (instance, job))
        * 100 > 1
      for: 15m
      labels:
        severity: '2'
    - alert: KubeClientErrors
      annotations:
        description: 'Kubernetes API server client "{{ $labels.job }}/{{ $labels.instance
          }}" is experiencing {{ printf "%0.0f" $value }} errors / second.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclienterrors
      expr: |
        sum(rate(ksm_scrape_error_total{job="kube-state-metrics"}[5m])) by (instance, job) > 0.1
      for: 15m
      labels:
        severity: '2'
    - alert: KubeletTooManyPods
      annotations:
        description: 'Kubelet {{ $labels.node }} is running {{ $value }} Pods, close
          to the limit of 110.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubelettoomanypods
      expr: |
        kubelet_running_pod_count{job="kubelet", node!~"gke-app-imf.*"} > 110 * 0.9
      for: 15m
      labels:
        severity: '2'
    - alert: KubeletTooManyPods
      annotations:
        description: 'Kubelet {{ $labels.node }} is running {{ $value }} Pods, close
          to the limit of 200.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubelettoomanypods
      expr: |
        kubelet_running_pod_count{job="kubelet", node=~"gke-app-imf.*"} > 200 * 0.9
      for: 15m
      labels:
        severity: '2'
    - alert: KubeAPILatencyHigh
      annotations:
        description: 'The API server has a 99th percentile latency of {{ $value }} seconds
          for {{ $labels.verb }} {{ $labels.resource }}.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh
      expr: |
        cluster_quantile:apiserver_request_latencies:histogram_quantile{job="apiserver",quantile="0.99",subresource!="log",verb!~"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$"} > 1
      for: 10m
      labels:
        severity: '2'
    - alert: KubeAPILatencyHigh
      annotations:
        description: 'The API server has a 99th percentile latency of {{ $value }} seconds
          for {{ $labels.verb }} {{ $labels.resource }}.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh
      expr: |
        cluster_quantile:apiserver_request_latencies:histogram_quantile{job="apiserver",quantile="0.99",subresource!="log",verb!~"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$"} > 4
      for: 10m
      labels:
        severity: '3'
    - alert: KubeAPIErrorsHigh
      annotations:
        description: 'API server is returning errors for {{ $value }}% of requests.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
      expr: |
        sum(rate(apiserver_request_count{job="apiserver",code=~"^(?:5..)$"}[5m])) without(instance, pod)
          /
        sum(rate(apiserver_request_count{job="apiserver"}[5m])) without(instance, pod) * 100 > 10
      for: 10m
      labels:
        severity: '3'
    - alert: KubeAPIErrorsHigh
      annotations:
        description: 'API server is returning errors for {{ $value }}% of requests.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
      expr: |
        sum(rate(apiserver_request_count{job="apiserver",code=~"^(?:5..)$"}[5m])) without(instance, pod)
          /
        sum(rate(apiserver_request_count{job="apiserver"}[5m])) without(instance, pod) * 100 > 5
      for: 10m
      labels:
        severity: '2'

  - name: alertmanager.rules
    rules:
    - alert: AlertmanagerConfigInconsistent
      annotations:
        description: 'The configuration of the instances of the Alertmanager cluster `{{$labels.service}}`
          are out of sync.'
      expr: |
        count_values("config_hash", alertmanager_config_hash{job="alertmanager-main"}) BY (service) / ON(service) GROUP_LEFT() label_replace(prometheus_operator_spec_replicas{job="prometheus-operator",controller="alertmanager"}, "service", "alertmanager-$1", "name", "(.*)") != 1
      for: 5m
      labels:
        severity: '2'
    - alert: AlertmanagerFailedReload
      annotations:
        description: 'Reloading Alertmanagers configuration has failed for {{ $labels.namespace
          }}/{{ $labels.pod}}.'
      expr: |
        alertmanager_config_last_reload_successful{job="alertmanager-main"} == 0
      for: 10m
      labels:
        severity: '3'

  - name: general.rules
    rules:
    # Majority of targets should be pods, but they don't have to be (for example nodes).
    # If targets are pods, then we try to route them to appropriate teams, other targets are routed to infra team
    - alert: TargetDown
      annotations:
        description: '{{ $value }}% of the {{ $labels.namespace }}/{{ $labels.job }} pods are down.'
      expr: |
        100 *
        (count(up{pod!=""} * on(namespace,pod) group_left(label_team) kube_pod_labels == 0) by (namespace,job,label_team)
          /
        count(up{pod!=""} * on(namespace,pod) group_left(label_team) kube_pod_labels) by (namespace,job,label_team))
        > 10
      for: 10m
      labels:
        severity: '2'
    - alert: TargetDown
      annotations:
        description: '{{ $value }}% of the {{ $labels.namespace}}/{{ $labels.job }} targets are down.'
      expr: 100 * (count(up{pod=""} == 0) BY (job, namespace) / count(up{pod=""}) BY (job, namespace)) > 10
      for: 10m
      labels:
        severity: '2'
    - alert: DeadMansSwitch
      annotations:
        description: This is a DeadMansSwitch meant to ensure that the entire alerting
          pipeline is functional.
      expr: vector(1)
      labels:
        severity: '0'
    - alert: ProbeFailed
      annotations:
        description: 'Failed probe: {{ $labels.target }}'
      expr: probe_success == 0
      for: 1m
      labels:
        severity: '3'

  - name: prometheus.rules
    rules:
    - alert: PrometheusConfigReloadFailed
      annotations:
        description: 'Reloading Prometheus configuration has failed for {{$labels.namespace}}/{{$labels.pod}}'
        summary: Reloading Promehteus' configuration failed
      expr: |
        prometheus_config_last_reload_successful{job="prometheus-k8s"} == 0
      for: 10m
      labels:
        severity: '1'
    - alert: PrometheusNotificationQueueRunningFull
      annotations:
        description: 'Prometheus alert notification queue is running full for {{$labels.namespace}}/{{
          $labels.pod}}'
        summary: Prometheus alert notification queue is running full
      expr: |
        predict_linear(prometheus_notifications_queue_length{job="prometheus-k8s"}[5m], 60 * 30) > prometheus_notifications_queue_capacity{job="prometheus-k8s"}
      labels:
        severity: '1'
    - alert: PrometheusErrorSendingAlerts
      annotations:
        description: 'Errors while sending alerts from Prometheus {{$labels.namespace}}/{{
          $labels.pod}} to Alertmanager {{$labels.Alertmanager}}'
        summary: Errors while sending alert from Prometheus
      expr: |
        rate(prometheus_notifications_errors_total{job="prometheus-k8s"}[5m]) / rate(prometheus_notifications_sent_total{job="prometheus-k8s"}[5m]) > 0.01
      for: 10m
      labels:
        severity: '2'
    - alert: PrometheusErrorSendingAlerts
      annotations:
        description: 'Errors while sending alerts from Prometheus {{$labels.namespace}}/{{
          $labels.pod}} to Alertmanager {{$labels.Alertmanager}}'
        summary: Errors while sending alerts from Prometheus
      expr: |
        rate(prometheus_notifications_errors_total{job="prometheus-k8s"}[5m]) / rate(prometheus_notifications_sent_total{job="prometheus-k8s"}[5m]) > 0.03
      for: 10m
      labels:
        severity: '3'
    - alert: PrometheusNotConnectedToAlertmanagers
      annotations:
        description: 'Prometheus {{ $labels.namespace }}/{{ $labels.pod}} is not connected
          to any Alertmanagers'
        summary: Prometheus is not connected to any Alertmanagers
      expr: |
        prometheus_notifications_alertmanagers_discovered{job="prometheus-k8s"} < 1
      for: 10m
      labels:
        severity: '1'
    - alert: PrometheusTSDBReloadsFailing
      annotations:
        description: '{{$labels.job}} at {{$labels.instance}} had {{$value | printf "%.0f" | humanize}}
          reload failures over the last four hours.'
        summary: Prometheus has issues reloading data blocks from disk
      expr: |
        increase(prometheus_tsdb_reloads_failures_total{job="prometheus-k8s"}[2h]) > 0
      for: 12h
      labels:
        severity: '1'
    - alert: PrometheusTSDBCompactionsFailing
      annotations:
        description: '{{$labels.job}} at {{$labels.instance}} had {{$value | printf "%.0f" | humanize}}
          compaction failures over the last four hours.'
        summary: Prometheus has issues compacting sample blocks
      expr: |
        increase(prometheus_tsdb_compactions_failed_total{job="prometheus-k8s"}[2h]) > 0
      for: 12h
      labels:
        severity: '1'
    - alert: PrometheusTSDBWALCorruptions
      annotations:
        description: '{{$labels.job}} at {{$labels.instance}} has a corrupted write-ahead
          log (WAL).'
        summary: Prometheus write-ahead log is corrupted
      expr: |
        tsdb_wal_corruptions_total{job="prometheus-k8s"} > 0
      for: 4h
      labels:
        severity: '1'
    - alert: PrometheusNotIngestingSamples
      annotations:
        description: 'Prometheus {{ $labels.namespace }}/{{ $labels.pod}} isnt ingesting
          samples.'
        summary: Prometheus isn't ingesting samples
      expr: |
        rate(prometheus_tsdb_head_samples_appended_total{job="prometheus-k8s"}[5m]) <= 0
      for: 10m
      labels:
        severity: '1'
    - alert: PrometheusTargetScrapesDuplicate
      annotations:
        description: '{{$labels.namespace}}/{{$labels.pod}} has many samples rejected
          due to duplicate timestamps but different values'
        summary: Prometheus has many samples rejected
      expr: |
        increase(prometheus_target_scrapes_sample_duplicate_timestamp_total{job="prometheus-k8s"}[5m]) > 0
      for: 10m
      labels:
        severity: '1'

  - name: prometheus-operator
    rules:
    - alert: PrometheusOperatorReconcileErrors
      annotations:
        description: 'Errors while reconciling {{ $labels.controller }} in {{ $labels.namespace
          }} Namespace.'
      expr: |
        rate(prometheus_operator_reconcile_errors_total{job="prometheus-operator"}[5m]) > 0.1
      for: 10m
      labels:
        severity: '1'
    - alert: PrometheusOperatorNodeLookupErrors
      annotations:
        description: 'Errors while reconciling Prometheus in {{ $labels.namespace }} Namespace.'
      expr: |
        rate(prometheus_operator_node_address_lookup_errors_total{job="prometheus-operator"}[5m]) > 0.1
      for: 10m
      labels:
        severity: '1'
